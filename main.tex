
%%%%%%%%%%%%%%%%%%%%%%%% PREAMBLE
\documentclass[]{beamer}
% \usepackage{pgfpages}
% \usepackage{handoutWithNotes}
% \pgfpagesuselayout{4 on 1 with notes}[a4paper,border shrink=5mm]
% \pgfpagesuselayout{4 on 1}[a4paper,border shrink=5mm]
%%%%%%%%%%%%%%%%%%%%%%%% PREAMBLE
\usepackage[T1]{fontenc}
\UseRawInputEncoding
\usepackage{graphicx, calligra}
\graphicspath{ {./figs/} }
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{graphicx,listings,stackengine}
\usepackage{BGU}
\usepackage{todonotes}
\presetkeys{todonotes}{inline}{}  % for beamer

% defs
\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.55}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{deepblue},
    emphstyle=\ttfamily\color{deepred},    % Custom highlighting style
    stringstyle=\color{deepgreen},
    numbers=left,
    numberstyle=\small\color{halfgray},
    rulesepcolor=\color{red!20!green!20!blue!20},
    frame=shadowbox,
}

%%%%%%%%%%%%%%%%%%%%%%%% BEAMER NOTES
% https://gist.github.com/andrejbauer/ac361549ac2186be0cdb
% These slides also contain speaker notes. You can print just the slides,
% just the notes, or both, depending on the setting below. Comment out the want
% you want.

\setbeameroption{hide notes} % Only slides
% \setbeameroption{show only notes} % Only notes
% \setbeameroption{show notes on second screen=right} % Both

% To give a presentation with the Skim reader (http://skim-app.sourceforge.net) on OSX so
% that you see the notes on your laptop and the slides on the projector, do the following:
% 
% 1. Generate just the presentation (hide notes) and save to slides.pdf
% 2. Generate only the notes (show only nodes) and save to notes.pdf
% 3. With Skim open both slides.pdf and notes.pdf
% 4. Click on slides.pdf to bring it to front.
% 5. In Skim, under "View -> Presentation Option -> Synhcronized Noted Document"
%    select notes.pdf.
% 6. Now as you move around in slides.pdf the notes.pdf file will follow you.
% 7. Arrange windows so that notes.pdf is in full screen mode on your laptop
%    and slides.pdf is in presentation mode on the projector.

% Give a slight yellow tint to the notes page
\setbeamertemplate{note page}{\pagecolor{yellow!5}\insertnote}\usepackage{palatino}


%%%%%%%%%%%%% BEAMER SETTINGS %%%%%%%%%%%%%%
% \AtBeginSection[]
% {
% \begin{frame}<beamer>
% \frametitle{Plan}
% \tableofcontents[currentsection]
% \end{frame}
% }

% \AtBeginSubsection[] % Do nothing for \subsection*
% {
% % \begin{frame}<beamer>
% % \frametitle{Outline}
% % \tableofcontents[currentsection,currentsubsection]
% % \end{frame}
% }

%%% MACROS
\usepackage{svg}
\usepackage{pdfpages}
\usepackage{amsmath}
\usepackage{bm}
\newcommand{\policy}{\ensuremath{\boldsymbol{\pi}}}

\theoremstyle{definition}
% \newtheorem{definition}{Definition}[section]
\newtheorem*{fct}{Fact}
\newtheorem*{hwv}{However}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
% Allows repeating frame contents (duplicating boxes)
% https://tex.stackexchange.com/questions/107237/repeating-frame-contents
\newcommand\dupbox[2][]{%
\begin{block}{\duptitle}
\begin{minipage}{\linewidth}
\duptext
\end{minipage}
\end{block}
\begin{block}{#1}
\begin{minipage}{\linewidth}
#2
\end{minipage}
\end{block}
\gdef\duptext{#2}
\gdef\duptitle{#1}
}

\gdef\duptext{Initial text}
\gdef\duptitle{}



%%%%% THEME
% \usetheme{Boadilla}
% \usetheme{Madrid}
% \usecolortheme{default}


%%%% HEADER
\title[Probabilistic Inference for Solving MDPs]{Probabilistic Inference for Solving \\Markov Decision Processes}
\subtitle{Toussaint, Storkey, ICML '06}
\author{Noam Siegel}
\institute[BGU]
{Computer Science M.Sc. Seminar\\
Ben Gurion University}
\date{\today}


%%%%%%%%%%%%% DOCUMENT %%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%% Title Page %%%%%%%%%%%%%%

\begin{frame}
	\titlepage
% 	\note[item]{Thank the audience for being awake.}
	\note[item]{Prepare markers and eraser, erase the board.}
	\note[item]{Reset the timer.}
\end{frame}


%%%%%%%%%%%%% TOC %%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Outline}
	\tableofcontents
\end{frame}

%%%%%%%%%%%%% DOCUMENT %%%%%%%%%%%%%%
\section{Introduction}

%%%%%%%%%%%%% Preliminaries %%%%%%%%%%%%%%

% \subsection{Prerequisites}

% \begin{frame}{Prerequisites}
%         \todo{todo: add prerequisites}
% % 		\todo{add illustration of Probabilistic Inference in Graphical Models}
% % 		\todo{add illustration of Dynamic Bayesian Networks}
%         \note{for example: probabilistic inference, graphical models, dynamic bayesian networks}
% \end{frame}

%%%%%%%%%%%%% Problem %%%%%%%%%%%%%%
\subsection{Motivation \& Prior work}
\begin{frame}<1-2>{Motivation}
    \setbeamercolor{background canvas}{bg=}
    \only<1|handout:0>{
    \includegraphics[width=\textwidth]{figs/fig_0a_starting.png}
    }
    \only<2|handout:1>{
    \includegraphics[width=\textwidth]{figs/fig_0b_starting.png}
    % \includepdf[]{figs/fig_0b_starting.pdf}
    }
    \note<1>[item]{Planning in stochastic environments, means solving a Markov Decision Process with a stochastic reward function.}
    \note<1>[item]{Inference in Markovian models, means using Markovian assumptions to make estimating the values of probabilistic queries tractable.}
    \note<1>[item]{similarities: both fields face the challenge of coping with very large state spaces spanned by multiple state variables, or working in continuous state spaces.}
	\note<2>[item]{The paper I am presenting to you bridges between the two fields, providing a solution to planning in stochastic environments using standard probabilistic inference techniques.}
\end{frame}

\begin{frame}{Prior work}
    \begin{enumerate}
        \item \emph{Bui et al. (2002)} used inference on Abstract Hidden Markov Models for policy recognition, \emph{but not for computing an optimal policy}.
        \item \emph{Attias (2003)} got close to translating the problem of planning to a problem of inference. However, \emph{the total time $T$ had to be fixed} and the MAP action sequence that is proposed as a solution \emph{is not optimal}.
        \item \emph{Verma and Rao (2006)} used inference to compute plans, but again $T$ has to be fixed and the plan is not optimal.
    \end{enumerate}
    
	\note[item]{existing ways to deal with challenges in each field: Factored MDPs or abstractions (in planning), approximate inference (inference).}
	\note[item]{describe and mention previous approaches to merging planning and inference, and mention their limitations.}
	\note[item]{Not optimal in the sense of maximizing an expected future reward}
\end{frame}

\subsection{Main contribution}
\begin{frame}{Main contribution}
	\begin{alertblock}{Contribution}
		\begin{enumerate}[<+-|alert@+>]
			\item Translate the problem of \emph{maximizing the expected future return} exactly into a problem of \emph{likelihood maximization in a latent variable model}, for arbitrary reward functions and episode lengths.
			\item Demonstrate the approach on discrete \& continuous stochastic optimal control problems,
		\end{enumerate}
	\end{alertblock}
	\note<1>[item]{first contrib.: provide a framework that translates the problem of maximizing the expected future return exactly into a problem of likelihood maximization in a latent variable mixture model, for arbitrary rewards functions and without assuming a fixed time}
	\note<2>[item]{second contrib.: demonstrate the approach first on a discrete maze problem, then on a continuous stochastic optimal control problem.}
	\note<2>[item]{In the discrete maze we will use exact inference in the E-step, and in the continuous case we will assume Gaussian belief state representations and use the unscented transform to handle non-linear dynamics.}
\end{frame}


\section{Research Problem}

\subsection{Solving Markov Decision Processes}

\begin{frame}{Examples 1: Discrete maze}
    \begin{figure}
        \centering
        \includegraphics[width=0.6\textwidth]{figs/fig_3a_maze_example.png}
        \caption{Posterior state-visiting-probabilities generated by our Probabilistic Infernce Planner (PIP)}
        \label{fig:my_label}
    \end{figure}
	\note[item]{This figure is the posterior state-visiting-probabilities generated by our probabilistic inference planner (PIP) for a problem where rewards are given when some goal state is reached.}
	\note[item]{This is the image I want you to have in mind.}
\end{frame}

\begin{frame}<1-3>{Markov Decision Processes}
	\only<3>{
	\includegraphics[width=\textwidth]{figs/fig_1_dbn_mdp.png}
	}
	\only<1, 2, 3>{
	\begin{definition}[MDP]
		\begin{smashedalign}
			\text{state transition probability } &P(x_{t+1} \mid a_t, x_t)\\
			\text{reward probability } &P(r_t \mid a_t; x_t), \quad \quad r_t \in \{0, 1\} \\
			\text{action probability } &P(a_t \mid x_t; \policy), \quad \quad \policy \text{ a parameter}
		\end{smashedalign}
	\end{definition}
	}
	\only<2>{
	\begin{definition}[Policy \policy{}]
	    The action probabilities are parameterized by a policy:
	    \begin{equation*}
	         P(a_t \mid x_t = i; \policy) = \pi_{ai} \quad \quad s.t. \sum_a \pi_{ai} = 1
	    \end{equation*}
	\end{definition}
	}
	\note<1>[item]{This is an unrolled Dynamic Bayesian Network. Each node is a random variable, and each arrow denotes a causal relationship.}
    \note<1>[item]{The graph encodes conditional independence relations between sets of variables. Examples ($a_1 \bot a_0 \mid x_1$). These conditional independencies are what allow us to factor the probability distribution into fewer factors.}
	\note<1>[item]{The r.v.s $x$ and $a$ can be either discrete or continuous}
	\note<1>[item]{The reward variable $r$ is, w.l.g. assumed to be binary, $r_t \in \{0, 1\}$}
	\note<2>[item]{So, let's remember the definition of a Markov Decision Process:}
	\note<2>[item]{State transition probability is the same as usual, where the states and actions can be either continuous or discrete.}
	\note<2>[item]{The reward probability is stochastic, and the reward is binary, without loss of generality. Because we can get every expected value between 0 and 1, and rescale if need.}
	\note<2>[item]{The action probability is dependent on a parameter \policy which we call the Policy.}
	\note<2>[item]{Throughout this paper we assume that the state transition and reward probabilities are given to us, i.e. known a priori.}
	\note<3>[item]{The action probability $P(a_t \mid x_t ; \policy)$ is parameterized by a policy \policy{} parameter such that $P(a_t \mid x_t = i; \policy) = \pi_{ai}$}
	\note<1>[item]{+++: w.l.g.: Assuming binary reward variables is sufficient to associate arbitrary reward expectations $P(r_t \mid a_t, x_t)$ in the interval $[0,1]$ to states and actions, which is, modulo rescaling, the general case in Reinforcement Learning scenarios.}
% 	\note{DRAW Figure 1 on the board!}
\end{frame}


\begin{frame}{Research problem}
	\begin{definition}[solving an MDP]
	\label{def:solve_mdp}
		\emph{Solving an MDP} means to find a parameter $\policy$ of the graphical model in Figure 1 that maximizes the expected future return $V^{\policy}(i) = E\{\sum_{t=0}^\infty \gamma^t r_t \mid x_0=i ; \policy \}$, where $\gamma \in [0, 1]$ is a discount factor.
	\end{definition}
	\pause
	\begin{alertblock}{research problem}
	The problem is to \emph{solve the MDP}, i.e. to find a policy that maximizes the expected future return.
	\end{alertblock}
	\note<1>[item]{This definition is an equivalent definition to the standard definition, since reductions can be made in both directions.}
	\note<2>[item]{The problem is to \emph{solve the MDP}, i.e., to find a policy that maximizes the expected future return}
	\note<2>[item]{The classical approach to solving MDPs is anchored in Bellman's equation, which simply reflects the recursive property of the future discounted return $R_T = \sum_{t=T}^{\infty} \gamma^t r_t = r_T + \gamma R_{T+1}$ and consequently of its expectation conditioned on the current state, $V_{\policy} = \sum_{j,a} P(j \mid a, i) P(a \mid i ; \policy) [P(r_t = 1 \mid a, i) + \gamma V^{\policy}(j)]$.}
	\note<2>[item]{Standard algorithms for computing value functions can be viewed as iterative schemes that converge towards the Bellman equation.}
	\note{WRITE $V^{\policy}(i) = E\{\sum_{t=0}^\infty \gamma^t r_t \mid x_0=i ; \policy \}$ on the board}
\end{frame}

\section{Research Plan}

\subsection{Mixture of MDPs and likelihood}
% \begin{frame}[shrink]{Mixture of MDPs}
% \includegraphics{figs/fig_2_mixture_mdps.png}
% \end{frame}

\makeatletter
\frame{{Mixture of MDPs}
	\global\beamer@shrinktrue
	\gdef\beamer@shrinkframebox{
		\setbox\beamer@framebox=\vbox to\beamer@frametextheight{
			\centering
			\includegraphics[height=\beamer@frametextheight]{figs/fig_2_mixture_mdps.png}
		}
	}
	\note[item]{Our approach is to cast the problem of solving an MDP into a problem of optimizing the parameters of a graphical model with many hidden variables, namely all future states and actions.}
	\note[item]{The only observables are the current state and rewards}
	\note[item]{To achieve exact equivalence between solving the MDP and standard likelihood maximization we will need to define a likelihood that is proportional to the expected future return.}
	\note[item]{This is most easily done by considering a \emph{mixture of finite-time MDPs}.}
	\note[item]{By a finite-time MDP we mean one which is limited in time by T, and which emits a reward variable only at the very final time step, as illustrated for various T in figure 2.}
	\note{DRAW Figure 2. on the board!}
}
\makeatother

\begin{frame}{Representing the joint distribution $P(\mathcal{X})$}
	\begin{block}{full joint for finite time MDP}
		\begin{smashedalign}
			  & P(r, x_{0:T}, a_{0:T} \mid T ; \policy) = \\& P(r \mid a_T, x_T)P(a_0 \mid x_0 ; \policy) P(x_0) \cdot \prod_{t=1}^{T}{P(a_t \mid x_t ; \policy) P(x_t \mid a_{t-1}, x_{t-1}})
		\end{smashedalign}
		
	\end{block}
	\pause
	\begin{block}{full joint for mixture of finite-time MDPs}
		$P(r,x_{0:T}, a_{0:T}, T ; \policy) = P(r, x_{0:T}, a_{0:T} \mid T ; \policy) P(T)$
	\end{block}
	\pause
	\begin{block}{prior over the total time}
	$P(T) = \gamma^T(1-\gamma)$
	\end{block}
	
	
	\note<1>[item]{This reads: The probability of seeing a reward $r$, a state sequence $x_{0:T}$, and an action sequence $a_{0:T}$, given an episode length $T$ and a policy $\policy$, can be factored as the product of multiple conditional probabilities.}
	\note<1>[item]{In this case we condition on the last variable ($r$), the first variables $a_0, x_0$, and for each time step in between there's a constant number of variables conditioned at each time step, thanks to the Markov property.}
% 	\note<1>[item]{This means we can write it down as a product of the probability of seeing a reward $r$ conditioned on the final state $x_T$ and action $a_T$, the probability of performing an action $a_0$ at state $x_0$ under policy $\policy$, the probability of observing state $x_0$, and for each $t \in \{1, .., T\}$ the probability of performing action $a_t$ at state $x_t$ under policy $\policy$, times the probability of transitioning from $x_{t-1}$ to $x_t$ if the action performed was $a_{t-1}$.}
	\note[item]<2>{This reads: using the chain rule, the joint probability of the observed reward, state sequence and action sequence, at a general time T, parameterized by the policy $\policy$, is a product of the joint for finite time MDP we just derived, and the prior probability we give to  $T$. And what prior do we choose?}
	\note[item]<3>{In this project we choose the prior over the total time, $P(T)$, such that the relative counts are proportional to the discounting.}
	\note[item]<3>{Note that each finite-time MDP shares the same transition probabilities and is parameterized by the same policy $\policy$}
	\note<3->{WRITE $P(T) = \gamma^T(1-\gamma)$ on board}
\end{frame}

\begin{frame}{Defining the likelihood}
    \pause
    \begin{definition}[likelihood for a finite-time MDP]
        $L_T^{\policy}(i) = P(r=1 \mid x_0=i, T ; \policy) = E\{r \mid x_0 = i, T ; \policy\}$
    \end{definition}
    \pause
    \begin{definition}[likelihood for mixture of MDPs]
        $L^{\policy}(i) = P(r = 1 \mid x_0 = i; \policy) = \sum\limits_T P(T)E\{r \mid x_0 = i, T ; \policy\}$
    \end{definition}
    \pause
    \begin{corollary}
        $L_T^{\policy}(i) = (1 - \gamma) V^{\policy}(i)$
    \end{corollary}
    \note<1->[item]{What we are trying to do is solve an MDP, which is to find a policy that maximizes the future reward. The approach here is to define a likelihood we we would like to maximize, and by maximizing the likelihood we will also maximize the expected future reward.}
    \note<2->[item]{For a sequence length $T$, the time-bounded MDP allows us to formulate a likelihood proportional to the expected future return for that run.}
    \note<3->[item]{The mixture of time-bounded MDPs allows us a likelihood proportional to the expected future return for any run, regardless of the episode length.}
    \note<4->[item]{This likelihood is, for the discounted time prior $P(T) = \gamma^T(1-\gamma)$, proportional to the expected discounted future return.}
    \note<4->[item]{+++: Note that the expectation term $E\{r \mid x_0 = i, T; \policy\}$ here is exactly the same as the terms $E\{r_t \mid x_0 = i ; \policy\}$ for $t=T$ in definition \ref{def:solve_mdp}: we are taking the expectation w.r.t. a full probabilistic forward-sweep through the MDP, from time 0 to time T, given the policy \policy. All MDPs share the same transition probabilities}
\end{frame}

\begin{frame}{Theoretical Guarantee}
	\begin{block}{Reminders}
	    \begin{enumerate}
	        \item \emph{Solving an MDP} means to find a parameter $\policy$ of the graphical model in Figure 1 that maximizes the expected future return $V^{\policy}(i) = E\{\sum_{t=0}^\infty \gamma^t r_t \mid x_0=i ; \policy \}$.
	        \item The \emph{likelihood for a mixture of MDPs} is given by $L^{\policy}(i) = P(r = 1 \mid x_0 = i; \policy) = \sum\limits_T P(T)E\{r \mid x_0 = i, T ; \policy\}$
	        \item This implies that $L_T^{\policy}(i) = (1 - \gamma) V^{\policy}(i)$
	    \end{enumerate}
	\end{block}
    \pause
    \begin{theorem}{(proved)}
        Maximizing the likelihood in the mixture of finite-time MDPs is equivalent to solving the MDP.
    \end{theorem}
    \note<1->[item]{Let us remember what we've seen so far.}
    \note<2->[item]{Thus, we have establised an important theoretical guarantee: there is an exact equivalence between the maximization of the likelihood and expected future return.}
    \note<2->[item]{Note that considering the mixture of finite-time models has a second advantage: the finite-time property of every mixture component makes the E-step in the full mixture model rather simple and efficient, as detailed in the next section.}
\end{frame}


\subsection{An EM-algorithm for computing the optimal policy}
\begin{frame}{An EM-algorithm for computing the optimal policy}
    \begin{block}{What are EM algorithms?}
        A class of algorithms consisting of two modes:
        \begin{enumerate}[<alert@+>]
            \item E-step: estimates missing variables.
            \item M-step: optimizes parameters of model to best explain the data.
        \end{enumerate}
    \end{block}
    \pause
    \begin{block}{E-step}
        For a given \policy{}, compute posteriors $P(T \mid x_0 = A, r = 1; \policy)$ and $P(x_{1:T}, a_{1:T} \mid x_0 = A, r = 1, T; \policy)$
    \end{block}
    \pause
    \begin{block}{M-step}
        Adapt parameters $\policy$ to optimize $V^{\policy}(A)$
    \end{block}
	\note[item]{The EM algorithm (or family of algorithms) is an iterative approach that cycles between two modes. The first mode attempts to estimate the missing or latent variables, called the estimation step or E-step. The second mode attempts to optimize the parameters of the model to best explain the data, called the maximization-step or M-step.}
	\note[item]{Formulating the objective function in terms of a likelihood allows us to apply Expectation-Maximization to find optimal parameters (the policy \policy{}) of our model. All action and state variables (except for $x_0$ are hidden variables).}
	\note[item]{The E-step will, for a given $\policy$, compute posteriors over state-action sequences as well as $T$ conditioned on $x_0 = A$ and $r=1$}
	\note[item]{The M-step then adapts the model parameters $\policy$ to optimize the expected likelihood (expectations then taken w.r.t. the posteriors calculated in the E-step.}
	\note[item]{Conceptually, the E-step in this Markovian model is straight-forward. However, the special structure of the finite-time MDPs will allow for certain simplifications and save us from performing separate inference sweeps in all finite-time MDPs}
\end{frame}

\begin{frame}<1-4>{E-step: forward-backward in all MDPs synchronously}
    \only<1>{
    \begin{block}{Simplifying notation}
       \begin{enumerate}
        \item $p(j \mid a, i) = P(x_{t+1} = j \mid a_t = a, x_t = i)$
        \item $p(j \mid i; \policy) = P(x_{t+1} = j \mid x_t = i ; \policy) = \sum_a p(j \mid a, i) \pi_{ai}$
    \end{enumerate}  
    \end{block}
    }
    
    \only<2,3>{
    % \begin{definition}["seed" for backward propagation]
    % $\hat{\beta}(i) = P(r = 1 \mid x_T = i ; \policy) = \sum\limits_a P(r=1 \mid a_T=a, x_T = i) \pi_{ai}$
    % \end{definition}
    
    \begin{block}{Forward Propagation}
        \begin{columns}[b]
               \begin{column}{3cm}
                       $\alpha_0(i) = \delta_{i=A}$
               \end{column}
               \begin{column}{7cm}
                \begin{smashedalign}
                    \alpha_t(i) &= P(x_t=i \mid x_0 = A ; \policy)\\
                        &= \sum\limits_j p(i \mid j ; \policy) \alpha_{t-1}(j)
                \end{smashedalign}
               \end{column}
        \end{columns}
    \end{block}
    }
    \only<3->{
    \begin{block}{Backward Propagation (temp.)}
        \begin{columns}[b]
               \begin{column}{3cm}
                       $\tilde{\beta}_T(i) = \hat{\beta}(i)$
               \end{column}
               \begin{column}{7cm}
                \begin{smashedalign}
                    \tilde{\beta}_t(i) &= P(r = 1 \mid x_t = i ; \policy)\\
                        &= \sum\limits_j p(j \mid i ; \policy) \tilde{\beta}_{t+1}(j)
                \end{smashedalign}
               \end{column}
        \end{columns}
        Where $\hat{\beta}(i) = P(r = 1 \mid x_T = i ; \policy) = \sum\limits_a P(r=1 \mid a_T=a, x_T = i) \pi_{ai}$
    \end{block}
    }
    
    % \only<4->{
    % \begin{block}{Backward Propagation (temp.)}
    %     \begin{columns}[b]
    %           \begin{column}{3cm}
    %                   $\tilde{\beta}_T(i) = \hat{\beta}(i)$
    %           \end{column}
    %           \begin{column}{7cm}
    %             \begin{smashedalign}
    %                 \tilde{\beta}_t(i) &= P(r = 1 \mid x_t = i ; \policy)\\
    %                     &= \sum\limits_j p(j \mid i ; \policy) \tilde{\beta}_{t+1}(j)
    %             \end{smashedalign}
    %           \end{column}
    %     \end{columns}
    %     Where $\hat{\beta}(i) = P(r = 1 \mid x_T = i ; \policy) = \sum\limits_a P(r=1 \mid a_T=a, x_T = i) \pi_{ai}$
    % \end{block}
    % }
    \only<4>{
    \begin{block}{Backward Propagation (corrected)}
        \begin{columns}[b]
               \begin{column}{3cm}
                       $\beta_0(i) = \hat{\beta}(i)$
               \end{column}
               \begin{column}{7cm}
                \begin{smashedalign}
                    \beta_\tau(i) &= P(r = 1 \mid x_{T - \tau} = i ; \policy)\\
                        &= \sum\limits_j p(j \mid i ; \policy) \beta_{\tau - 1}(j)
                \end{smashedalign}
               \end{column}
        \end{columns}
        % Here $\hat{\beta}(i)$ is unchanged.
    \end{block}
    }

    \note<1>[item]{Since we assume the transition probabilities to be stationary, we may use the simpler notations.}
    \note<2, 3>[item]{We will only show the algorithm for the discrete case, but I promise you that the EM algorithm works also for the continuous case. Examples in the paper.}
    \note<2, 3>[item]{Specifically, for the discrete case, the algorithm used is called the forward-backward algorithm. It is a form of belief propagation through message passing.}
    \note<2, 3>[item]{In the E-step, we consider a fixed given policy \policy{} and all the quantities we compute depend on \policy{} even if not explicitly annotated.}
    \note<2, 3>[item]{For a single MDP of finite time T we perform a forward and backward propagation as shown. $\alpha_t$ is the "location" belief state at time $t$, i.e., the probability distribution over states, conditioned on the initial state and parameterized by policy \policy{}.).
    }
    \note<3->[item]{$\beta_t$ is the reward belief state, i.e. we assume that at time $t$ we are at state $x_t$, and we ask what is the probability of observing a reward, parameterized by policy \policy{}. $\hat{\beta}$ is the starting "seed" for the process. And then we go backward one time step at a time, using $\beta_{t+1}$ to calculate $\beta_t$.}
    \note<3, 4>[item]{We find that all the $\alpha$-quantities do not depend on $T$ in any way, i.e., they are valid for all MDPs of any finite time $T$. This is not true for the $\tilde{\beta}$-quantities when defined as above.}
    \note<4>[item]{However, we can use a simple trick, namely define the $\beta$'s to be indexed backward in time (with the 'time-to-go' $\tau$), and get:}
    \note<4>[item]{Defined in that way, all $\beta$-quantities do indeed not depend on $T$. For a specific MDP of finite time $T$, setting $\tau = T - t$ would allow us to retrieve the traditional forward-indexed $\tilde{\beta}$-quantities.}
    \note<4>[item]{This means that we can perform $\alpha$- and $\beta$- propagation in parallel, incrementing $t$ and $\tau$ synchronously, and can retrieve the $\alpha$'s and $\beta$'s for all MDPs of any finite time $T$. Although we introduce a mixture of MDPs we only have to perform a single forward and backward sweep. This procedure is, from the point of view of ordinary HMMs, quite unusual - it is possible because in our specific setup we only condition on the very first ($x_0 = A$) and very last state ($r = 1$).}
    % \note<4>[item]{Apart from the implementation of discounting with the time prior, this is the main reason why we considered the mixture of finite-time MDPs in the first place, instead of a single time-unbounded MDP that could emit rewards at any times (figure 1).}
\end{frame}

% \begin{frame}<1-2,4->
% This is slide number \only<1, 2>{1}\only<2>{2}\only<3>{3}%
% \only<4>{4}\only<5>{5}.
% \end{frame}


\begin{frame}{M-step: the policy update}
    \only<1-3>{
	\begin{definition}[expected complete log-likelihood]
	$Q(\pi^*, \pi) = \sum_T \sum_{x_{0:T}, a_{0:T}} P(x_{0:T}, a_{0:T}, T \mid r = 1; \policy)$
	\end{definition}
	}
	\only<2-3>{
	\begin{fct}
	    Maximizing $Q(\pi^*, \pi)$ w.r.t. $\pi^*$ is achieved by setting\\$\pi_{ai}^* = P(a_t = a \mid x_t = i, r=1; \policy)$
	\end{fct}
	}
	\only<3>{
	\begin{hwv}
	exploiting the structure of the MDP, we can also write the likelihood as
	\begin{smashedalign}
	    P(r=1 \mid x_0=i; \policy) = &\sum\limits_{aj} P(r=1 ; a_t=a, x_t = j ; \policy^*) \pi_{aj}^*\\&\cdot P(x_t = j \mid x_0 = i ; \policy^*)
	\end{smashedalign}
	\end{hwv}
	}
	\only<4>{
	\begin{hwv}
    	exploiting the structure of the MDP, we can also write the likelihood as
    	\begin{smashedalign}
    	    P(r=1 \mid x_0=i; \policy) = &\sum\limits_{aj} P(r=1 ; a_t=a, x_t = j ; \policy^*) \pi_{aj}^*\\&\cdot P(x_t = j \mid x_0 = i ; \policy^*)
    	\end{smashedalign}
	\end{hwv}
	\begin{block}{}
	     \begin{columns}[b]
              \begin{column}{2.5cm}
                      $\pi_{ai}^{*} = \delta_{a=a^{*}(i)}$
              \end{column}
              \begin{column}{7.5cm}
                    $\alpha^{*}(i) = \underset{a}{\argmax} P(r=1 \mid a_t=a, x_t=i; \policy)$
              \end{column}
        \end{columns}
	\end{block}

    }
	
	\note<1->[item]{The standard M-step in an EM-algorithm maximizes the \emph{expected complete log-likelihood} w.r.t. the new parameters $\pi^*$, where expectations over the latent variables $(T, x_{0:T}, a_{0:T}$ were taken w.r.t. the posterior given the old parameters $\pi$.}
	\note<2->[item]{In our case, in strong analogy to the standard HMM case, this turns out to assign the new parameters to the posterior action probability}
	\note<3->[item]{Maximizing this expression w.r.t. $\pi_{aj}^*$ can be done separately for every $j$ and is thus independent of $P(x_t = j \mid x_0 = i ; \policy^*)$ and thereby also independent of $t$ because $P(r= 1 \mid a_t = a, x_t = j; \policy^*)$ is so.}
	\note<4->[item]{Using the E-step we can approximate the first term and maximize $\sum_a P(r = 1 \mid a_t = a, x_t = j; \policy) \pi_{aj}^*$ via:}
	\note<4->[item]{where $a^*(i)$ maximizes the action-conditioned likelihood. Given the relations between equations 15\&16, the main difference to the standard M-step is that this update is much greedier and converges faster in the MDP case. We will use this update rather than the previous one shown in the experiments.}
	\note<4->[item]{This is the standard policy update in Policy Iteration (Sutton \& Barto, 1998)}
\end{frame}

\subsection{Relation to Policy Iteration}
\begin{frame}{Relation to Policy Iteration}
    \only<1-6>{
    \begin{block}{E-step: Policy Evaluation}
        \setbeamercolor{itemize/enumerate body}{fg=gray}
         \begin{enumerate}[<+-|alert@+>]
             \item $\beta_\tau(i) \propto (V^{\policy}(i) \textit{ of the MDP of time } T = \tau)$.
             \item $V^{pi}(i) = \frac{1}{1 - \gamma} \sum_T P(T) \beta_T(i)$.
             \item Hence, \emph{the E-step performs a policy evaluation}.
             \item Additionally, \emph{it yields the time, state and action posteriors}.
         \end{enumerate}
    \end{block}
    }
    \only<6>{
    \begin{block}{M-step: Policy Update}
         \begin{enumerate}[<alert@+>]
             \item Maximizing the Q-function w.r.t. the action $a$ and state $i$.
         \end{enumerate}
    \end{block}
    }
    \only<7-8>{
    \begin{alertblock}{Thus,}
         the EM-algorithm using exact inference and belief representation is  effectively equivalent to Policy Iteration but computes the necessary quantities in a different way.
    \end{alertblock}
    }
    \only<8>{
    \begin{alertblock}{However,}
         when using approximate inference or belief representations 
    \end{alertblock}
    }
    \note[item]{The $\beta$-quantities computed during backward-propagation are actually the value function for a single MDP of finite time $T$.}
    \note[item]{More precisely, comparing eq (9) with the reward likelihood (3) for the MDP of finite time T and the and the reward definition 2.1 of the value function, we have that \emph{the $\beta$-quantity with $\tau$ time to go at the ($i$th state) is proportional to the Value function of the MDP at time T=$\tau$.}}
    \note[item]{Accordingly, the full value function is the mixture of the $\beta$'s, when a discount time prior is chosen}
    \note[item]{Additionally, yields ... which have no traditional analogue.}
    
\end{frame}

\section{Experimental results}

\subsection{Discrete maze}



\makeatletter
\frame{{Discrete maze}
	\global\beamer@shrinktrue
	\gdef\beamer@shrinkframebox{
		\setbox\beamer@framebox=\vbox to\beamer@frametextheight{
			\centering
			\includegraphics[height=\beamer@frametextheight]{figs/fig_3a_maze_example.png}
		}
	}
	\note[item]{We tested our probabilistic inference planning (PIP) algorithm on a discrete maze of size 100 X 100 and compared it to standard value iteration (VI) and policy iteration (PI).}
	\note[item]{Walls of the maze are considered to be trap states (leading to unsucessful trials) and actions (north, south, east, west, stay) are highly noisy in that with a probability of 0.2 they lead to random transitions. In the experiment we chose a uniform time prior (discount factor $\gamma = 1$ and iterated the policy update $k=5$ times.}
	\note[item]{To increase computational efficiency we exploited that the algorithm explicitly calculates posteriors which can be used to prune unnecessary message passings as explained in appendix A.}
	\note[item]{For policy evaluation in PI we performed 100 iterations of standard value function updates.}
	\note[item]{Figure 3(a) displays the posterior state visitng probabilities generated by our probabilistic inference planner (PIP) for a problem where rewards are given when some goal state is reached. }
}
\makeatother


\makeatletter
\frame{{Discrete maze}
	\global\beamer@shrinktrue
	\gdef\beamer@shrinkframebox{
		\setbox\beamer@framebox=\vbox to\beamer@frametextheight{
			\centering
			\includegraphics[height=\beamer@frametextheight]{figs/fig_3b_maze_example.png}
		}
	}
	\note[item]{Computational costs are measure by the number of evaluations of the environment $p(j \mid i, a)$ needed during the planning procedure. Figure 3(b) displays the probability of reaching the goal $P(r=1 \mid x_) = A; \policy$ against these costs for the same two start/goal state configurations.}
	\note[item]{Note that for PIP (and PI) we can give this information only after a complete E- and M-step cycle (policy evaluation and update) which are the discrete dots (triangles) in the graph.}
	\note[item]{The graph also displays the curve for VI, where the currently calculated value $V_A$ of the start state (which converges to $P(B \mid A)$ for the optimal policy) is plotted against how often VI evaluated $p(j \mid i, a)$.}
	\note[item]{In contrast to VI and PI, the PIP algorithm takes considerable advantage of knowing the start state in this planning scenario: the forward propagation allows for the pruning and the early decision on cutoff times of the E-step as described in Appendix A. It should thus not be a surprise and not overstated that PIP is significantly more efficient in this specific scenario. Certainly, some kind of forward propagations could also be introduced for VI or PI to achieve similar efficiency. Nonetheless, our approach provides a principled way of pruning by exploiting the computation of proper posterios.}
	\note[item]{A detailed inspection of the policies computed by all three methods showed that they are equal for states which have significantly non-zero state visiting probabilities.}
}
\makeatother

\subsection{Stochastic optimal control}

\begin{frame}{Stochastic optimal control - 'walker'}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{figs/fig_4ab_control_example.png}
        % \caption{}
        \label{fig:my_label}
    \end{figure}
    \vspace*{-0.6cm}
    \begin{alertblock}{}
    \begin{itemize}
        \item $P(x' \mid u, x) = \mathcal{N}(x', \phi(u, x), Q + (|u|/\mu)^2 I)$ is \emph{transitions}.
        \item $\alpha_0(x) = \mathcal{N}(x, (0,0), .01I)$ \quad \quad \quad \quad \quad \quad \quad is   \emph{start-state}.
        \item $P(r=1 \mid x) = \mathcal{N}(x, (1,1), diag(.0001,.1))$ is \emph{goal}.
        \item $\phi(u, x) = x + .1u$ \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad is \emph{control-law}.
    \end{itemize}
    \end{alertblock}
    
    \note[item]{We address the problem of \emph{stochastic optimal control} in the case of a \emph{continuous state and control space}. (both continuous state and continuous control).}
    \note[item]{Figure 4ab shows the learned control policy \policy{} and the forward simulation given this policy by displaying the covariance ellipses for $\alpha_{0:T}(x)$ after $k=3$ EM iterations.}
    \note[item]{\emph{What we find is a control policy that reduces errors in $x$-dimension more strongly than in $y$-direction, leading to a tangential approach.}}
    \note[item]{As a transition model we assume .(eq). where $\phi(u, x)$ is a non-linear function, $Q$ is a constant noise covariance, and we introduced a parameter $\mu$ for an additional noise term that is squared in the control signal.}
    \note[item]{We assume Gaussian belief states as representations for $\alpha$'s and $\beta$'s and propagate forward-backward.}
    \note[item]{This goal region is heavily skewed in that rewards depend more on the precision in the x-dimension than the y-dimension.}
    \note[item]{When choosing $\mu=0$, (no control-dependent noise), the optimal control policy will try to jumpy directly to the goal $(1,1)$. Hence, we first consider the solution when manually constraining the norm $|u|$ to be small ('walker')}
    \note[item]{This example shows that the framework naturally allows to transfer other inference techniques to the problem of solving MDPs.}
    \note[item]{+++:use the unscented transform to handle also non-linear transition dynamics (see K. Murphy Ph.D. 2002).}
    \note[item]{+++:using Gaussian belief states implies that the effective value function (Section 5) becomes a mixture of Gaussians.}
\end{frame}


\begin{frame}{Stochastic optimal control - 'golfer'}
    \includegraphics[width=\textwidth]{figs/fig_4cd_control_example.png}
\end{frame}


\begin{frame}{Stochastic optimal control - 'phase space'}
    \includegraphics[width=\textwidth]{figs/fig_4ef_control_example.png}
\end{frame}



\section{Conclusion}
\begin{frame}{Conclusion}
	\begin{block}{}
	    We present a model that translates the problem of planning into a problem of probabilistic inference.
	\end{block}
	\pause
	\begin{block}{Main contributions}
	     \begin{enumerate}
	         \item we do not have to fix a total time
	         \item likelihood maximization is equivalent to maximization of the expected future return
 	   \end{enumerate}
	\end{block}
	\pause
	\begin{block}{}
	     We can compute posteriors over actions, states, and the total time.
	\end{block}
	\pause
	\begin{block}{}
	     The full variety of existing inference techniques can be applied to solving MDPs.
	\end{block}
	\note[item]{The key step to our approach is to introduce a mixture of finite-time MDPs as a model that is equivalent to the original time-unbounded MPD but has two advantages: it allows us to formulate a likelihood proportional to the expected future return, and inference in the mixture model can efficiently be realized with a single synchronous forward-backward propagation without having to fix a finite total time in advance.}
\end{frame}

\begin{frame}<beamer>{End}
	\begin{center}
		{\Huge\calligra Thank You}
	\end{center}
\end{frame}

\end{document}

% references
% https://www.cs.ubc.ca/~murphyk/Papers/dbnchapter.pdf